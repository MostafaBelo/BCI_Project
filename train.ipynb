{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156af74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import CLIPModel, CLIPProcessor, \\\n",
    "                            DistilBertModel, DistilBertTokenizerFast, \\\n",
    "                            GPT2Tokenizer, GPT2Model, \\\n",
    "                            RobertaTokenizer, RobertaModel, \\\n",
    "                            AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "                            pipeline\n",
    "import google.generativeai as genai\n",
    "#genai.configure(api_key=\"\")\n",
    "\n",
    "from torcheeg.models import EEGNet\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from EEGDataset import EEGDataset, WordEEGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6036bfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9812",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fbea55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1076, 4000]),\n",
       " '0',\n",
       " 'Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds = WordEEGDataset(\"shards\", pad_upto=200)\n",
    "ds = EEGDataset(\"shards\", pad_upto=4000, crp_rng=(0,1))\n",
    "ds[0][0].shape, ds[0][1], ds[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd170d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(274)))\n",
    "# val_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(274,332)))\n",
    "# test_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(332,392)))\n",
    "\n",
    "# train_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(40)))\n",
    "# val_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(40,45)))\n",
    "# test_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(45,49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54197c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 300, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = ds.split_train_valid_test(train_ratio=0.7, valid_ratio=0.15, shuffle=False)\n",
    "\n",
    "train_dl = train_ds.getLoader(batch_size=5, num_workers=0)\n",
    "val_dl = val_ds.getLoader(batch_size=5, num_workers=0)\n",
    "test_dl = test_ds.getLoader(batch_size=5, num_workers=0)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b74a97",
   "metadata": {},
   "source": [
    "# Text Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19975b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:626\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    624\u001b[39m     progress.update(progress_bytes)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     85\u001b[39m         last_hidden = outputs.hidden_states[-\u001b[32m1\u001b[39m]\n\u001b[32m     86\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m last_hidden[:, \u001b[32m0\u001b[39m, :]\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m model = \u001b[43mTextEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mTextEncoder.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     19\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mj-hartmann/sentiment-roberta-large-english-3-classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4902\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4907\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4913\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:1066\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1064\u001b[39m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[32m   1065\u001b[39m         filename = _add_variant(WEIGHTS_NAME, variant)\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(WEIGHTS_NAME, variant):\n\u001b[32m   1070\u001b[39m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[32m   1071\u001b[39m     resolved_archive_file = cached_file(\n\u001b[32m   1072\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   1073\u001b[39m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[32m   1074\u001b[39m         **cached_file_kwargs,\n\u001b[32m   1075\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1168\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1181\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1720\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1719\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1720\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1728\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:621\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    610\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(…)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m progress_cm = _get_progress_bar_context(\n\u001b[32m    613\u001b[39m     desc=displayed_filename,\n\u001b[32m    614\u001b[39m     log_level=logger.getEffectiveLevel(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m     _tqdm_bar=_tqdm_bar,\n\u001b[32m    619\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    624\u001b[39m         progress.update(progress_bytes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/tqdm/std.py:1138\u001b[39m, in \u001b[36mtqdm.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self):\n",
    "        # self.model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "        # self.tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "        # self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        # self.tokenizer = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        # self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        # self.model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "        self.model = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "\n",
    "        # self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        # self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "        # model_name = \"j-hartmann/sentiment-roberta-large-english-3-classes\"\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def encode(self, txts):\n",
    "        # all-MiniLM-L6-v2\n",
    "        # return torch.tensor(self.model.encode(txts))\n",
    "    \n",
    "        # distilbert-base-uncased\n",
    "        # inputs = self.tokenizer(\n",
    "        #     txts,\n",
    "        #     truncation=True,\n",
    "        #     padding=True,\n",
    "        #     max_length=256,\n",
    "        #     return_tensors=\"pt\"\n",
    "        # )\n",
    "        # return self.model(**inputs).last_hidden_state[:,0]\n",
    "\n",
    "        # openai/clip-vit-base-patch32\n",
    "        # inputs = self.tokenizer(\n",
    "        #     text=txts,\n",
    "        #     padding=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # return self.model.get_text_features(**inputs)\n",
    "\n",
    "        # gpt2\n",
    "        # inputs = self.tokenizer(\n",
    "        #     txts,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # last_hidden = self.model(**inputs).last_hidden_state\n",
    "        # return last_hidden[torch.arange(last_hidden.size(0)), inputs['attention_mask'].sum(1)-1]\n",
    "        # # attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        # # text_embeds_mean = (last_hidden * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "        # # return text_embeds_mean / text_embeds_mean.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # paraphrase-mpnet-base-v2\n",
    "        return torch.tensor(self.model.encode(txts))\n",
    "\n",
    "        # Google Gemini\n",
    "        # res = genai.embed_content(\n",
    "        #     model=\"models/text-embedding-004\",\n",
    "        #     content=txts,\n",
    "        #     task_type=\"retrieval_document\"\n",
    "        # )[\"embedding\"]\n",
    "        # return torch.tensor(res)\n",
    "\n",
    "        # Roberta\n",
    "        # inputs = self.tokenizer(txts, return_tensors=\"pt\")\n",
    "        # token_embeddings = self.model(**inputs).last_hidden_state\n",
    "        # mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        # summed = torch.sum(token_embeddings * mask, dim=1)\n",
    "        # counted = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "        # return summed / counted\n",
    "\n",
    "        # inputs = self.tokenizer(\n",
    "        #     txts,\n",
    "        #     padding=True,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\"\n",
    "        # )\n",
    "        # outputs = self.model.roberta(**inputs, output_hidden_states=True)\n",
    "        # last_hidden = outputs.hidden_states[-1]\n",
    "        # return last_hidden[:, 0, :]\n",
    "\n",
    "model = TextEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904eca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 768]), torch.float32, torch.Tensor)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embeddings = model.encode([\"Negative\", \"Neutral\", \"Positive\"])\n",
    "target_embeddings.shape, target_embeddings.dtype, type(target_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959278e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.61        36\n",
      "           1       0.30      0.31      0.31        32\n",
      "           2       0.60      0.50      0.55        30\n",
      "\n",
      "    accuracy                           0.49        98\n",
      "   macro avg       0.49      0.48      0.49        98\n",
      "weighted avg       0.49      0.49      0.49        98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lbls = []\n",
    "txts = []\n",
    "txt_embeds = []\n",
    "for i in tqdm(range(100)):\n",
    "    _, sent, txt = ds[i]\n",
    "    if _ is None:\n",
    "        continue\n",
    "    lbls.append(int(sent)+1)\n",
    "    txts.append(txt)\n",
    "    txt_embeds.append(model.encode([txt]))\n",
    "\n",
    "preds = torch.cat(txt_embeds, dim=0)\n",
    "\n",
    "# cat_norm = target_embeddings.clone().detach()\n",
    "# text_norm = preds.clone().detach()\n",
    "# diffs = torch.cdist(text_norm, cat_norm, p=2)\n",
    "# pred = diffs.argmin(dim=1)\n",
    "\n",
    "cat_norm = F.normalize(target_embeddings, p=2, dim=1)\n",
    "text_norm = F.normalize(preds, p=2, dim=1)\n",
    "similarities = torch.matmul(text_norm, cat_norm.T)\n",
    "pred = similarities.argmax(dim=1)\n",
    "\n",
    "print(classification_report(lbls, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd109d2",
   "metadata": {},
   "source": [
    "# Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b811dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1076, 10000]),\n",
       " '0',\n",
       " 117,\n",
       " 'Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape, train_ds[0][1], len(train_ds[0][2]), train_ds[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6496de3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1076, 10000]), 120)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[0][0].shape, len(val_ds[0][2])\n",
    "# val_ds[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9beda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 384), dtype('float32'), numpy.ndarray)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings = model.encode([ds[0][1]])\n",
    "embeddings = model.encode([train_ds[0][1]])\n",
    "embeddings.shape, embeddings.dtype, type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9248eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 torch.Size([10, 1076, 10000])\n",
      "10 torch.Size([10])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "ds[0][0].shape\n",
    "\n",
    "for batch_data, batch_sent, batch_labels in train_dl:\n",
    "    print(len(batch_data), batch_data.shape)\n",
    "    print(len(batch_sent), batch_sent.shape)\n",
    "    print(len(batch_labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbb368",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb981eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        # self.text_encoder_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        # self.text_encoder_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "        # self.processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "        self.text_encoder_model = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "\n",
    "    def forward(self, texts):\n",
    "        # inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "        # inputs = {k: v.to(self.text_encoder_model.device) for k, v in inputs.items()}\n",
    "        # embeddings = self.text_encoder_model.get_text_features(**inputs)\n",
    "        \n",
    "        embeddings = self.text_encoder_model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, ch_count=8196, embedding_dim=384):\n",
    "        super(EEGEncoder, self).__init__()\n",
    "\n",
    "        dropprob = .3\n",
    "\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv1d(ch_count, 1024, 64, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Conv1d(1024, 512, 32, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Conv1d(512, 256, 32, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            # nn.AdaptiveAvgPool2d((256, 1))\n",
    "            nn.AdaptiveAvgPool1d((1))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, embedding_dim//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Linear(embedding_dim//2, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # self.eeg_encoder = EEGNet(\n",
    "        #     chunk_size=5,\n",
    "        #     num_electrodes=ch_count,\n",
    "        #     dropout=dropprob,\n",
    "        #     kernel_1=5,\n",
    "        #     kernel_2=5,\n",
    "        #     F1=8,\n",
    "        #     F2=16,\n",
    "        #     D=2,\n",
    "        #     num_classes=embedding_dim\n",
    "        # )\n",
    "\n",
    "    def compute_power_bands(self, x):\n",
    "        fs = 500\n",
    "        \n",
    "        # eeg: (N, C, T)\n",
    "        freqs = torch.fft.rfftfreq(x.size(-1), 1/fs).to(x.device)  # (F,)\n",
    "        fft_vals = torch.fft.rfft(x, dim=-1)                         # (N, C, F)\n",
    "        psd = (fft_vals.abs()**2)\n",
    "\n",
    "        bands = [(0.5,4), (4,8), (8,12), (12,30), (30,49)]\n",
    "        feats = []\n",
    "        for low, high in bands:\n",
    "            idx = (freqs >= low) & (freqs < high)\n",
    "            band = psd[..., idx].mean(dim=-1)   # (N, C)\n",
    "            feats.append(band)\n",
    "\n",
    "        return torch.stack(feats, dim=2)        # (N, C, P)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.fft.rfft(x, dim=2)\n",
    "        x = torch.log(torch.abs(x) + 1e-8)\n",
    "\n",
    "        # x = self.compute_power_bands(x)\n",
    "\n",
    "        x = self.temporal(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # x = x.unsqueeze(1)\n",
    "        # x = self.eeg_encoder(x)\n",
    "\n",
    "        # x = F.normalize(x, p=2, dim=1)\n",
    "        # x = torch.tanh(x) * 3\n",
    "        return x\n",
    "    \n",
    "class EEGClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=384):\n",
    "        super(EEGClassifier, self).__init__()\n",
    "\n",
    "        dropprob = .3\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Linear(16, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "class EEGCLIPModel(nn.Module):\n",
    "    def __init__(self, ch_count=8196, embedding_dim=384, freeze_text=True):\n",
    "        super(EEGCLIPModel, self).__init__()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.eeg_encoder = EEGEncoder(ch_count=ch_count, embedding_dim=embedding_dim)\n",
    "        self.eeg_classifier = EEGClassifier(embedding_dim=embedding_dim)\n",
    "\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, eeg_data, texts):\n",
    "        eeg_embeddings = self.eeg_encoder(eeg_data)\n",
    "        text_embeddings = self.text_encoder(texts)\n",
    "        classification = self.eeg_classifier(eeg_embeddings)\n",
    "        return eeg_embeddings, text_embeddings, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157eea70",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb52d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EEGCLIPModel().to(device)\n",
    "model = EEGCLIPModel(1076, 768, freeze_text=True).to(device)\n",
    "# model.load_state_dict(torch.load(\"best_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg, sent, text = ds[0]\n",
    "res = model(eeg.unsqueeze(0).to(torch.float32).to(\"cuda\"), [text])\n",
    "\n",
    "res[0].shape, res[1].shape, res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Embedding Normalization Space\n",
    "all_embeds = model.text_encoder([lbl[1] for lbl in ds.labels])\n",
    "\n",
    "text_mean = all_embeds.mean(dim=0)\n",
    "text_std = all_embeds.std(dim=0).clamp(min=1e-6)\n",
    "\n",
    "text_mean, text_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Classes\n",
    "classification_classes = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "classification_classes_embeds = model.text_encoder(classification_classes)\n",
    "classification_classes_embeds_z = (classification_classes_embeds-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "classification_classes_embeds_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "827f34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_loader: DataLoader, valid_loader: DataLoader, epochs: int = 10):\n",
    "# def train(model: nn.Module, train_dataset: WordEEGDataset, valid_dataset: WordEEGDataset, epochs: int = 10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_valid_loss = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_count = 0\n",
    "        train_correct = 0\n",
    "        train_total_count = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "        # for batch in tqdm(train_dataset):\n",
    "            # if isinstance(train_dataset, EEGDataset):\n",
    "            #     batch = (batch[0].unsqueeze(0), [batch[1]])\n",
    "            # batch = ([batch[0]], [batch[1]])\n",
    "            B = batch[0].shape[0]\n",
    "            eeg_data = batch[0].to(torch.float32).clone().detach().to(device)\n",
    "            sent_lbl = batch[1].to(torch.int64).clone().detach().to(device)\n",
    "            texts = batch[2]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            eeg_embeddings, text_embeddings, sent_logits = model(eeg_data, texts)\n",
    "            sent_logits: torch.Tensor\n",
    "\n",
    "            loss = 0\n",
    "            loss += 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "            loss += loss_fn(sent_logits, sent_lbl) / 2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_count += 1\n",
    "\n",
    "            sent_probs = sent_logits.softmax(dim=1)\n",
    "            sent_preds = sent_probs.argmax(dim=1)\n",
    "            train_correct += (sent_lbl == sent_preds).sum().item()\n",
    "            train_total_count += B\n",
    "\n",
    "            # for i in range(len(batch[1])):\n",
    "            #     eeg_data = batch[0][i].to(torch.float32).to(device)\n",
    "            #     texts = batch[1][i]\n",
    "\n",
    "            #     optimizer.zero_grad()\n",
    "            #     eeg_embeddings, text_embeddings = model(eeg_data, texts)\n",
    "\n",
    "            #     # text_z = (text_embeddings-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "            #     # loss = loss_fn(eeg_embeddings, text_z)\n",
    "            #     loss = 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "            #     loss.backward()\n",
    "            #     optimizer.step()\n",
    "\n",
    "            #     total_loss += loss.item()\n",
    "            #     train_count += 1\n",
    "\n",
    "            #     # Accuracy Check\n",
    "            #     # text_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "            #     # eeg_norm = F.normalize(eeg_embeddings, p=2, dim=1)\n",
    "            #     # classes_norm = F.normalize(classification_classes_embeds, p=2, dim=1)\n",
    "            #     sim_ground = text_embeddings @ classification_classes_embeds.T\n",
    "            #     sim_pred = eeg_embeddings @ classification_classes_embeds.T\n",
    "\n",
    "            #     # sim_ground = -torch.cdist(text_z.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "            #     # sim_pred = -torch.cdist(eeg_embeddings.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "\n",
    "            #     ground = sim_ground.argmax(dim=1)\n",
    "            #     pred = sim_pred.argmax(dim=1)\n",
    "            #     train_correct += (ground == pred).sum().item()\n",
    "            #     train_total_count += ground.shape[0]\n",
    "\n",
    "\n",
    "        # avg_loss = total_loss / len(train_loader)\n",
    "        avg_loss = total_loss / train_count\n",
    "        avg_acc = train_correct / train_total_count\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_valid_loss = 0.0\n",
    "        valid_count = 0\n",
    "        valid_correct = 0\n",
    "        valid_total_count = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch in tqdm(valid_loader):\n",
    "            # for batch in tqdm(valid_dataset):\n",
    "                # if isinstance(valid_dataset, EEGDataset):\n",
    "                #     batch = (batch[0].unsqueeze(0), [batch[1]])\n",
    "                # batch = ([batch[0]], [batch[1]])\n",
    "                B = batch[0].shape[0]\n",
    "                eeg_data = batch[0].to(torch.float32).clone().detach().to(device)\n",
    "                sent_lbl = batch[1].to(torch.int64).clone().detach().to(device)\n",
    "                texts = batch[2]\n",
    "\n",
    "                eeg_embeddings, text_embeddings, sent_logits = model(eeg_data, texts)\n",
    "                sent_logits: torch.Tensor\n",
    "\n",
    "                loss = 0\n",
    "                loss += 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "                loss += loss_fn(sent_logits, sent_lbl) / 2\n",
    "\n",
    "                total_valid_loss += loss.item()\n",
    "                valid_count += 1\n",
    "\n",
    "                sent_probs = sent_logits.softmax(dim=1)\n",
    "                sent_preds = sent_probs.argmax(dim=1)\n",
    "                valid_correct += (sent_lbl == sent_preds).sum().item()\n",
    "                valid_total_count += B\n",
    "\n",
    "                # for i in range(len(batch[0])):\n",
    "                #     eeg_data = batch[0][i].to(torch.float32).to(device)\n",
    "                #     texts = batch[1][i]\n",
    "\n",
    "                #     eeg_embeddings, text_embeddings = model(eeg_data, texts)\n",
    "\n",
    "                #     # text_z = (text_embeddings-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "                #     # loss = loss_fn(eeg_embeddings, text_z)\n",
    "                #     loss = 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "\n",
    "                #     total_valid_loss += loss.item()\n",
    "                #     valid_count += 1\n",
    "\n",
    "                #     # Accuracy Check\n",
    "                #     # text_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "                #     # eeg_norm = F.normalize(eeg_embeddings, p=2, dim=1)\n",
    "                #     # classes_norm = F.normalize(classification_classes_embeds, p=2, dim=1)\n",
    "                #     sim_ground = text_embeddings @ classification_classes_embeds.T\n",
    "                #     sim_pred = eeg_embeddings @ classification_classes_embeds.T\n",
    "\n",
    "                #     # sim_ground = -torch.cdist(text_z.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "                #     # sim_pred = -torch.cdist(eeg_embeddings.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "\n",
    "                #     ground = sim_ground.argmax(dim=1)\n",
    "                #     pred = sim_pred.argmax(dim=1)\n",
    "                #     valid_correct += (ground == pred).sum().item()\n",
    "                #     valid_total_count += ground.shape[0]\n",
    "                    \n",
    "        # avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "        avg_valid_loss = total_valid_loss / valid_count\n",
    "        avg_valid_acc = valid_correct / valid_total_count\n",
    "\n",
    "        if (best_valid_loss is None) or (avg_valid_loss < best_valid_loss):\n",
    "            print(f\"Valid Loss: {avg_valid_loss:.10f}\")\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]:- Train Loss: {avg_loss:.6f} | Train Acc: {avg_acc*100:.4f}% | Valid Loss: {avg_valid_loss:.6f} | Valid Acc: {avg_valid_acc*100:.4f}%\")\n",
    "        torch.save(model.state_dict(), \"last_model.pt\")    \n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e93c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_classes = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "\n",
    "def test(model: nn.Module, test_loader: DataLoader):\n",
    "# def test(model: nn.Module, test_dataset: WordEEGDataset):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    actuals = []\n",
    "    preds = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_loader):\n",
    "        # for batch in tqdm(test_dataset):\n",
    "            # if isinstance(test_dataset, EEGDataset):\n",
    "            #     batch = (batch[0].unsqueeze(0), [batch[1]])\n",
    "            # batch = ([batch[0]], [batch[1]])\n",
    "            B = batch[0].shape[0]\n",
    "            eeg_data = batch[0].to(torch.float32).clone().detach().to(device)\n",
    "            sent_lbl = batch[1].to(torch.int64).clone().detach().to(device)\n",
    "            texts = batch[2]\n",
    "\n",
    "            eeg_embeddings, text_embeddings, sent_logits = model(eeg_data, texts)\n",
    "            sent_logits: torch.Tensor\n",
    "\n",
    "            loss = 0\n",
    "            loss += 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "            loss += loss_fn(sent_logits, sent_lbl)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "            sent_probs = sent_logits.softmax(dim=1)\n",
    "            sent_preds = sent_probs.argmax(dim=1)\n",
    "            correct += (sent_lbl == sent_preds).sum().item()\n",
    "            total_count += B\n",
    "\n",
    "            actuals.append(sent_lbl)\n",
    "            preds.append(sent_preds)\n",
    "            # for i in range(len(batch[0])):\n",
    "            #     eeg_data = batch[0][i].to(torch.float32).to(device)\n",
    "            #     texts = batch[1][i]\n",
    "\n",
    "            #     eeg_embeddings, text_embeddings = model(eeg_data, texts)\n",
    "\n",
    "            #     # text_z = (text_embeddings-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "            #     # loss = loss_fn(eeg_embeddings, text_z)\n",
    "            #     loss = 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "\n",
    "            #     total_loss += loss.item()\n",
    "            #     count += 1\n",
    "\n",
    "            #     # Accuracy Check\n",
    "            #     # text_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "            #     # eeg_norm = F.normalize(eeg_embeddings, p=2, dim=1)\n",
    "            #     # classes_norm = F.normalize(classification_classes_embeds, p=2, dim=1)\n",
    "            #     sim_ground = text_embeddings @ classification_classes_embeds.T\n",
    "            #     sim_pred = eeg_embeddings @ classification_classes_embeds.T\n",
    "\n",
    "            #     # sim_ground = -torch.cdist(text_z.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "            #     # sim_pred = -torch.cdist(eeg_embeddings.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "\n",
    "            #     ground = sim_ground.argmax(dim=1)\n",
    "            #     pred = sim_pred.argmax(dim=1)\n",
    "            #     correct += (ground == pred).sum().item()\n",
    "            #     total_count += ground.shape[0]\n",
    "            #     actuals.append(ground)\n",
    "            #     preds.append(pred)\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    avg_acc = correct / total_count\n",
    "    print(f\"Test Loss: {avg_loss:.6f} | Test Acc: {avg_acc*100:.4f}%\")\n",
    "\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    cr = classification_report(actuals.cpu(), preds.cpu(), labels=[0,1,2], target_names=classification_classes)\n",
    "    print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8b1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 112/280 [09:37<14:26,  5.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# train(model, train_ds, val_ds, epochs=3)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# train(model, ds, epochs=20)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, valid_loader, epochs)\u001b[39m\n\u001b[32m     22\u001b[39m texts = batch[\u001b[32m2\u001b[39m]\n\u001b[32m     24\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m eeg_embeddings, text_embeddings, sent_logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m sent_logits: torch.Tensor\n\u001b[32m     28\u001b[39m loss = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mEEGCLIPModel.forward\u001b[39m\u001b[34m(self, eeg_data, texts)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, eeg_data, texts):\n\u001b[32m    122\u001b[39m     eeg_embeddings = \u001b[38;5;28mself\u001b[39m.eeg_encoder(eeg_data)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     text_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     classification = \u001b[38;5;28mself\u001b[39m.eeg_classifier(eeg_embeddings)\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m eeg_embeddings, text_embeddings, classification\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mTextEncoder.forward\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True)\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# inputs = {k: v.to(self.text_encoder_model.device) for k, v in inputs.items()}\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# embeddings = self.text_encoder_model.get_text_features(**inputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_encoder_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     embeddings = F.normalize(embeddings, p=\u001b[32m2\u001b[39m, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1090\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1081\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[32m   1082\u001b[39m             features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat(\n\u001b[32m   1083\u001b[39m                 (\n\u001b[32m   1084\u001b[39m                     features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1087\u001b[39m                 -\u001b[32m1\u001b[39m,\n\u001b[32m   1088\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m features = \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/sentence_transformers/util/tensor.py:184\u001b[39m, in \u001b[36mbatch_to_device\u001b[39m\u001b[34m(batch, target_device)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         batch[key] = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dl, val_dl, epochs=5)\n",
    "# train(model, train_ds, val_ds, epochs=3)\n",
    "# train(model, ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc782275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:08<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.474045 | Test Acc: 81.0345%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.77      0.90      0.83        83\n",
      "     Neutral       0.89      0.54      0.67       100\n",
      "    Negative       0.81      0.99      0.89       107\n",
      "\n",
      "    accuracy                           0.81       290\n",
      "   macro avg       0.82      0.81      0.80       290\n",
      "weighted avg       0.82      0.81      0.80       290\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, test_dl)\n",
    "# test(model, test_ds)\n",
    "# test(model, ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
