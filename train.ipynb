{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156af74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import CLIPModel, CLIPProcessor, \\\n",
    "                            DistilBertModel, DistilBertTokenizerFast, \\\n",
    "                            GPT2Tokenizer, GPT2Model, \\\n",
    "                            RobertaTokenizer, RobertaModel, \\\n",
    "                            AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "                            pipeline\n",
    "# import google.generativeai as genai\n",
    "#genai.configure(api_key=\"\")\n",
    "\n",
    "from torcheeg.models import EEGNet\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from EEGDataset import EEGDataset, WordEEGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6036bfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9812",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fbea55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([105, 4000]),\n",
       " 1,\n",
       " 'Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds = WordEEGDataset(\"shards\", pad_upto=200)\n",
    "# ds = EEGDataset(\"shards\", ch_count=1076, pad_upto=6000, crp_rng=(0,1))\n",
    "ds = EEGDataset(\"shards\", ch_count=105, pad_upto=4000, crp_rng=(0,1))\n",
    "ds[0][0].shape, ds[0][1], ds[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd170d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(274)))\n",
    "# val_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(274,332)))\n",
    "# test_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(332,392)))\n",
    "\n",
    "# train_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(40)))\n",
    "# val_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(40,45)))\n",
    "# test_ds = WordEEGDataset(\"shards\", pad_upto=200, selective_indexing=list(range(45,49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54197c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3360, 720, 720)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = ds.split_train_valid_test(train_ratio=0.7, valid_ratio=0.15, shuffle=False)\n",
    "\n",
    "batch_size = 25\n",
    "train_dl = train_ds.getLoader(batch_size=batch_size, num_workers=0)\n",
    "val_dl = val_ds.getLoader(batch_size=batch_size, num_workers=0)\n",
    "test_dl = test_ds.getLoader(batch_size=batch_size, num_workers=0)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b74a97",
   "metadata": {},
   "source": [
    "# Text Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19975b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self):\n",
    "        # self.model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "        # self.tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "        # self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        # self.tokenizer = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        # self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        # self.model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "        self.model = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "\n",
    "        # self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        # self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "        # model_name = \"j-hartmann/sentiment-roberta-large-english-3-classes\"\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def encode(self, txts):\n",
    "        # all-MiniLM-L6-v2\n",
    "        # return torch.tensor(self.model.encode(txts))\n",
    "    \n",
    "        # distilbert-base-uncased\n",
    "        # inputs = self.tokenizer(\n",
    "        #     txts,\n",
    "        #     truncation=True,\n",
    "        #     padding=True,\n",
    "        #     max_length=256,\n",
    "        #     return_tensors=\"pt\"\n",
    "        # )\n",
    "        # return self.model(**inputs).last_hidden_state[:,0]\n",
    "\n",
    "        # openai/clip-vit-base-patch32\n",
    "        # inputs = self.tokenizer(\n",
    "        #     text=txts,\n",
    "        #     padding=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # return self.model.get_text_features(**inputs)\n",
    "\n",
    "        # gpt2\n",
    "        # inputs = self.tokenizer(\n",
    "        #     txts,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # last_hidden = self.model(**inputs).last_hidden_state\n",
    "        # return last_hidden[torch.arange(last_hidden.size(0)), inputs['attention_mask'].sum(1)-1]\n",
    "        # # attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        # # text_embeds_mean = (last_hidden * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "        # # return text_embeds_mean / text_embeds_mean.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # paraphrase-mpnet-base-v2\n",
    "        return torch.tensor(self.model.encode(txts))\n",
    "\n",
    "        # Google Gemini\n",
    "        # res = genai.embed_content(\n",
    "        #     model=\"models/text-embedding-004\",\n",
    "        #     content=txts,\n",
    "        #     task_type=\"retrieval_document\"\n",
    "        # )[\"embedding\"]\n",
    "        # return torch.tensor(res)\n",
    "\n",
    "        # Roberta\n",
    "        # inputs = self.tokenizer(txts, return_tensors=\"pt\")\n",
    "        # token_embeddings = self.model(**inputs).last_hidden_state\n",
    "        # mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        # summed = torch.sum(token_embeddings * mask, dim=1)\n",
    "        # counted = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "        # return summed / counted\n",
    "\n",
    "        # Roberta 3 Class\n",
    "        # inputs = self.tokenizer(\n",
    "        #     txts,\n",
    "        #     padding=True,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\"\n",
    "        # )\n",
    "        # outputs = self.model.roberta(**inputs, output_hidden_states=True)\n",
    "        # last_hidden = outputs.hidden_states[-1]\n",
    "        # return last_hidden[:, 0, :]\n",
    "\n",
    "model = TextEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904eca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 768]), torch.float32, torch.Tensor)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embeddings = model.encode([\"Negative\", \"Neutral\", \"Positive\"])\n",
    "target_embeddings.shape, target_embeddings.dtype, type(target_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959278e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [00:01<00:00, 38.12it/s]"
     ]
    }
   ],
   "source": [
    "lbls = []\n",
    "txts = []\n",
    "txt_embeds = []\n",
    "for i in tqdm(range(100)):\n",
    "    _, sent, txt = ds[i]\n",
    "    if _ is None:\n",
    "        continue\n",
    "    lbls.append(int(sent)+1)\n",
    "    txts.append(txt)\n",
    "    txt_embeds.append(model.encode([txt]))\n",
    "\n",
    "preds = torch.cat(txt_embeds, dim=0)\n",
    "\n",
    "# cat_norm = target_embeddings.clone().detach()\n",
    "# text_norm = preds.clone().detach()\n",
    "# diffs = torch.cdist(text_norm, cat_norm, p=2)\n",
    "# pred = diffs.argmin(dim=1)\n",
    "\n",
    "cat_norm = F.normalize(target_embeddings, p=2, dim=1)\n",
    "text_norm = F.normalize(preds, p=2, dim=1)\n",
    "similarities = torch.matmul(text_norm, cat_norm.T)\n",
    "pred = similarities.argmax(dim=1)\n",
    "\n",
    "print(classification_report(lbls, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd109d2",
   "metadata": {},
   "source": [
    "# Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b811dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([105, 4000]),\n",
       " 1,\n",
       " 117,\n",
       " 'Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape, train_ds[0][1], len(train_ds[0][2]), train_ds[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496de3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([105, 4000]), 210)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[0][0].shape, len(val_ds[0][2])\n",
    "# val_ds[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9beda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 384), dtype('float32'), numpy.ndarray)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings = model.encode([ds[0][1]])\n",
    "embeddings = model.encode([train_ds[0][1]])\n",
    "embeddings.shape, embeddings.dtype, type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([5, 105, 4000])\n",
      "5 torch.Size([5])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "ds[0][0].shape\n",
    "\n",
    "for batch_data, batch_sent, batch_labels in train_dl:\n",
    "    print(len(batch_data), batch_data.shape)\n",
    "    print(len(batch_sent), batch_sent.shape)\n",
    "    print(len(batch_labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbb368",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb981eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        # self.text_encoder_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        # self.text_encoder_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "        # self.processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "        self.text_encoder_model = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "\n",
    "    def forward(self, texts):\n",
    "        # inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "        # inputs = {k: v.to(self.text_encoder_model.device) for k, v in inputs.items()}\n",
    "        # embeddings = self.text_encoder_model.get_text_features(**inputs)\n",
    "        \n",
    "        embeddings = self.text_encoder_model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "    \n",
    "class EEGPatchEmbedding(nn.Module):\n",
    "    def __init__(self, n_channels=1076, patch_size=20, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        dropprob = 0.3\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(n_channels * patch_size, (n_channels * patch_size)//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Linear((n_channels * patch_size)//2,embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        B, C, T = x.shape\n",
    "        x = x[:, :, :T - (T % self.patch_size)]  # trim\n",
    "        x = x.view(B, C, -1, self.patch_size)    # (B, C, tokens, patch)\n",
    "        x = x.permute(0, 2, 1, 3)                 # (B, tokens, C, patch)\n",
    "        x = x.flatten(2)                          # (B, tokens, C*patch)\n",
    "        return self.proj(x)                       # (B, tokens, embed_dim)\n",
    "\n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels=1076,\n",
    "        patch_size=20,\n",
    "        embed_dim=384,\n",
    "        num_layers=4,\n",
    "        num_heads=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        dropprob = 0.3\n",
    "\n",
    "        self.patch_embed = EEGPatchEmbedding(\n",
    "            n_channels, patch_size, embed_dim\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * embed_dim,\n",
    "            dropout=dropprob,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        B = x.size(0)\n",
    "\n",
    "        x = self.patch_embed(x)   # (B, tokens, embed_dim)\n",
    "\n",
    "        cls = self.cls_token.expand(B, -1, -1) # FIXME:\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        cls_out = x[:, 0]\n",
    "\n",
    "        return cls_out\n",
    "\n",
    "\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, ch_count=8196, embedding_dim=384):\n",
    "        super(EEGEncoder, self).__init__()\n",
    "\n",
    "        dropprob = .3\n",
    "\n",
    "        # self.temporal = nn.Sequential(\n",
    "        #     nn.Conv1d(ch_count, 1024, 64, padding=1),\n",
    "        #     nn.LeakyReLU(),\n",
    "        #     nn.Dropout(dropprob),\n",
    "        #     nn.Conv1d(1024, 512, 32, padding=1),\n",
    "        #     nn.LeakyReLU(),\n",
    "        #     nn.Dropout(dropprob),\n",
    "        #     nn.Conv1d(512, 256, 32, padding=1),\n",
    "        #     nn.LeakyReLU(),\n",
    "        #     nn.Dropout(dropprob),\n",
    "        #     # nn.AdaptiveAvgPool2d((256, 1))\n",
    "        #     nn.AdaptiveAvgPool1d((1))\n",
    "        # )\n",
    "\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(256, embedding_dim//2),\n",
    "        #     nn.LeakyReLU(),\n",
    "        #     nn.Dropout(dropprob),\n",
    "        #     nn.Linear(embedding_dim//2, embedding_dim)\n",
    "        # )\n",
    "\n",
    "        # self.eeg_encoder = EEGNet(\n",
    "        #     chunk_size=3000,\n",
    "        #     num_electrodes=ch_count,\n",
    "        #     dropout=dropprob,\n",
    "        #     kernel_1=64,\n",
    "        #     kernel_2=16,\n",
    "        #     F1=8,\n",
    "        #     F2=16,\n",
    "        #     D=2,\n",
    "        #     num_classes=embedding_dim\n",
    "        # )\n",
    "\n",
    "        self.eeg_transformer = EEGTransformer(\n",
    "            n_channels=ch_count,\n",
    "            patch_size=200,\n",
    "            embed_dim=embedding_dim,\n",
    "            num_layers=4,\n",
    "            num_heads=8,\n",
    "        )\n",
    "\n",
    "    def compute_power_bands(self, x):\n",
    "        fs = 500\n",
    "        \n",
    "        # eeg: (N, C, T)\n",
    "        freqs = torch.fft.rfftfreq(x.size(-1), 1/fs).to(x.device)  # (F,)\n",
    "        fft_vals = torch.fft.rfft(x, dim=-1)                         # (N, C, F)\n",
    "        psd = (fft_vals.abs()**2)\n",
    "\n",
    "        bands = [(0.5,4), (4,8), (8,12), (12,30), (30,49)]\n",
    "        feats = []\n",
    "        for low, high in bands:\n",
    "            idx = (freqs >= low) & (freqs < high)\n",
    "            band = psd[..., idx].mean(dim=-1)   # (N, C)\n",
    "            feats.append(band)\n",
    "\n",
    "        return torch.stack(feats, dim=2)        # (N, C, P)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.fft.rfft(x, dim=2)\n",
    "        x = torch.log(torch.abs(x) + 1e-8)\n",
    "\n",
    "        # x = self.compute_power_bands(x)\n",
    "\n",
    "        # x = self.temporal(x).squeeze(-1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        # x = x.unsqueeze(1)\n",
    "        # x = self.eeg_encoder(x)\n",
    "\n",
    "        x = self.eeg_transformer(x)\n",
    "\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        # x = torch.tanh(x) * 3\n",
    "        return x\n",
    "    \n",
    "class EEGClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=384):\n",
    "        super(EEGClassifier, self).__init__()\n",
    "\n",
    "        dropprob = .3\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropprob),\n",
    "            nn.Linear(16, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "class EEGCLIPModel(nn.Module):\n",
    "    def __init__(self, ch_count=8196, embedding_dim=384, freeze_text=True):\n",
    "        super(EEGCLIPModel, self).__init__()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.eeg_encoder = EEGEncoder(ch_count=ch_count, embedding_dim=embedding_dim)\n",
    "        self.eeg_classifier = EEGClassifier(embedding_dim=embedding_dim)\n",
    "\n",
    "        if freeze_text:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, eeg_data, texts):\n",
    "        eeg_embeddings = self.eeg_encoder(eeg_data)\n",
    "        text_embeddings = self.text_encoder(texts)\n",
    "        classification = self.eeg_classifier(eeg_embeddings)\n",
    "        return eeg_embeddings, text_embeddings, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157eea70",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb52d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EEGCLIPModel().to(device)\n",
    "model = EEGCLIPModel(105, 768, freeze_text=False).to(device)\n",
    "# model = EEGCLIPModel(1076, 768, freeze_text=True).to(device)\n",
    "# model.load_state_dict(torch.load(\"last_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914d7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]),\n",
       " torch.Size([1, 768]),\n",
       " tensor([[-0.1902, -0.0113,  0.0710]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg, sent, text = ds[0]\n",
    "res = model(eeg.unsqueeze(0).to(torch.float32).to(\"cuda\"), [text])\n",
    "\n",
    "res[0].shape, res[1].shape, res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Embedding Normalization Space\n",
    "all_embeds = model.text_encoder([lbl[1] for lbl in ds.labels])\n",
    "\n",
    "text_mean = all_embeds.mean(dim=0)\n",
    "text_std = all_embeds.std(dim=0).clamp(min=1e-6)\n",
    "\n",
    "text_mean, text_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Classes\n",
    "classification_classes = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "classification_classes_embeds = model.text_encoder(classification_classes)\n",
    "classification_classes_embeds_z = (classification_classes_embeds-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "classification_classes_embeds_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1058e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_msg = \"\"\n",
    "def log(txt: str, end=\"\\n\"):\n",
    "    current_msg += txt + end\n",
    "\n",
    "    if len(current_msg) > 1000:\n",
    "        with open(\"logs.txt\", \"a\") as f:\n",
    "            f.write(current_msg)\n",
    "        current_msg = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827f34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, alpha, beta = 1, .7, 1.3\n",
    "loss_weights = (.5, 1, 1.5)\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, valid_loader: DataLoader, epochs: int = 10, print_every = 20):\n",
    "# def train(model: nn.Module, train_dataset: WordEEGDataset, valid_dataset: WordEEGDataset, epochs: int = 10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_valid_loss = None\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_count = 0\n",
    "        train_correct = 0\n",
    "        train_total_count = 0\n",
    "        for batch in train_loader:\n",
    "        # for batch in tqdm(train_loader):\n",
    "            if batch[0] is None:\n",
    "                continue\n",
    "        # for batch in tqdm(train_dataset):\n",
    "            # if isinstance(train_dataset, EEGDataset):\n",
    "            #     batch = (batch[0].unsqueeze(0), [batch[1]])\n",
    "            # batch = ([batch[0]], [batch[1]])\n",
    "            B = batch[0].shape[0]\n",
    "            eeg_data = batch[0].to(torch.float32).clone().detach().to(device) # (B,C,T)\n",
    "            sent_lbl = batch[1].to(torch.int64).clone().detach().to(device) # (B)\n",
    "            texts = batch[2] # [\"\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            eeg_embeddings, text_embeddings, sent_logits = model(eeg_data, texts) # (B,D)\n",
    "            sent_logits: torch.Tensor\n",
    "\n",
    "            cosine_loss = 2*(1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean())\n",
    "            ce_loss = loss_fn(sent_logits, sent_lbl)\n",
    "\n",
    "            sent_mask = sent_lbl.view(-1,1) == sent_lbl.view(1,-1)\n",
    "            dist = torch.cdist(eeg_embeddings, eeg_embeddings, p=2)\n",
    "            positive_losses = torch.relu(dist-tau*alpha)\n",
    "            negative_losses = torch.relu(tau*beta-dist)\n",
    "            positive_mask = torch.tril(sent_mask == True, -1)\n",
    "            negative_mask = torch.tril(sent_mask == False, -1)\n",
    "            if positive_mask.sum().item() == 0:\n",
    "                positive_loss = 0\n",
    "            else:\n",
    "                positive_loss = (positive_losses[positive_mask]).sum().item() / (positive_mask.sum().item())\n",
    "            if negative_mask.sum().item() == 0:\n",
    "                negative_loss = 0\n",
    "            else:\n",
    "                negative_loss = (negative_losses[negative_mask]).sum().item() / (negative_mask.sum().item())\n",
    "            clip_loss = positive_loss + negative_loss # clip loss\n",
    "            \n",
    "            loss = cosine_loss*loss_weights[0] + ce_loss*loss_weights[1] + clip_loss*loss_weights[2]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_count += 1\n",
    "\n",
    "            sent_probs = sent_logits.softmax(dim=1)\n",
    "            sent_preds = sent_probs.argmax(dim=1)\n",
    "            train_correct += (sent_lbl == sent_preds).sum().item()\n",
    "            train_total_count += B\n",
    "\n",
    "            # for i in range(len(batch[1])):\n",
    "            #     eeg_data = batch[0][i].to(torch.float32).to(device)\n",
    "            #     texts = batch[1][i]\n",
    "\n",
    "            #     optimizer.zero_grad()\n",
    "            #     eeg_embeddings, text_embeddings = model(eeg_data, texts)\n",
    "\n",
    "            #     # text_z = (text_embeddings-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "            #     # loss = loss_fn(eeg_embeddings, text_z)\n",
    "            #     loss = 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "            #     loss.backward()\n",
    "            #     optimizer.step()\n",
    "\n",
    "            #     total_loss += loss.item()\n",
    "            #     train_count += 1\n",
    "\n",
    "            #     # Accuracy Check\n",
    "            #     # text_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "            #     # eeg_norm = F.normalize(eeg_embeddings, p=2, dim=1)\n",
    "            #     # classes_norm = F.normalize(classification_classes_embeds, p=2, dim=1)\n",
    "            #     sim_ground = text_embeddings @ classification_classes_embeds.T\n",
    "            #     sim_pred = eeg_embeddings @ classification_classes_embeds.T\n",
    "\n",
    "            #     # sim_ground = -torch.cdist(text_z.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "            #     # sim_pred = -torch.cdist(eeg_embeddings.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "\n",
    "            #     ground = sim_ground.argmax(dim=1)\n",
    "            #     pred = sim_pred.argmax(dim=1)\n",
    "            #     train_correct += (ground == pred).sum().item()\n",
    "            #     train_total_count += ground.shape[0]\n",
    "\n",
    "\n",
    "        # avg_loss = total_loss / len(train_loader)\n",
    "        avg_loss = total_loss / train_count\n",
    "        avg_acc = train_correct / train_total_count\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_valid_loss = 0.0\n",
    "        valid_count = 0\n",
    "        valid_correct = 0\n",
    "        valid_total_count = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch in valid_loader:\n",
    "            # for batch in tqdm(valid_loader):\n",
    "                if batch[0] is None:\n",
    "                    continue\n",
    "            # for batch in tqdm(valid_dataset):\n",
    "                # if isinstance(valid_dataset, EEGDataset):\n",
    "                #     batch = (batch[0].unsqueeze(0), [batch[1]])\n",
    "                # batch = ([batch[0]], [batch[1]])\n",
    "                B = batch[0].shape[0]\n",
    "                eeg_data = batch[0].to(torch.float32).clone().detach().to(device)\n",
    "                sent_lbl = batch[1].to(torch.int64).clone().detach().to(device)\n",
    "                texts = batch[2]\n",
    "\n",
    "                eeg_embeddings, text_embeddings, sent_logits = model(eeg_data, texts)\n",
    "                sent_logits: torch.Tensor\n",
    "\n",
    "                cosine_loss = 2*(1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean())\n",
    "                ce_loss = loss_fn(sent_logits, sent_lbl)\n",
    "\n",
    "                sent_mask = sent_lbl.view(-1,1) == sent_lbl.view(1,-1)\n",
    "                dist = torch.cdist(eeg_embeddings, eeg_embeddings, p=2)\n",
    "                positive_losses = torch.relu(dist-tau*alpha)\n",
    "                negative_losses = torch.relu(tau*beta-dist)\n",
    "                positive_mask = torch.tril(sent_mask == True, -1)\n",
    "                negative_mask = torch.tril(sent_mask == False, -1)\n",
    "                if positive_mask.sum().item() == 0:\n",
    "                    positive_loss = 0\n",
    "                else:\n",
    "                    positive_loss = (positive_losses[positive_mask]).sum().item() / (positive_mask.sum().item())\n",
    "                if negative_mask.sum().item() == 0:\n",
    "                    negative_loss = 0\n",
    "                else:\n",
    "                    negative_loss = (negative_losses[negative_mask]).sum().item() / (negative_mask.sum().item())\n",
    "                clip_loss = positive_loss + negative_loss # clip loss\n",
    "\n",
    "                loss = cosine_loss*loss_weights[0] + ce_loss*loss_weights[1] + clip_loss*loss_weights[2]\n",
    "\n",
    "                total_valid_loss += loss.item()\n",
    "                valid_count += 1\n",
    "\n",
    "                sent_probs = sent_logits.softmax(dim=1)\n",
    "                sent_preds = sent_probs.argmax(dim=1)\n",
    "                valid_correct += (sent_lbl == sent_preds).sum().item()\n",
    "                valid_total_count += B\n",
    "\n",
    "                # for i in range(len(batch[0])):\n",
    "                #     eeg_data = batch[0][i].to(torch.float32).to(device)\n",
    "                #     texts = batch[1][i]\n",
    "\n",
    "                #     eeg_embeddings, text_embeddings = model(eeg_data, texts)\n",
    "\n",
    "                #     # text_z = (text_embeddings-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "                #     # loss = loss_fn(eeg_embeddings, text_z)\n",
    "                #     loss = 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "\n",
    "                #     total_valid_loss += loss.item()\n",
    "                #     valid_count += 1\n",
    "\n",
    "                #     # Accuracy Check\n",
    "                #     # text_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "                #     # eeg_norm = F.normalize(eeg_embeddings, p=2, dim=1)\n",
    "                #     # classes_norm = F.normalize(classification_classes_embeds, p=2, dim=1)\n",
    "                #     sim_ground = text_embeddings @ classification_classes_embeds.T\n",
    "                #     sim_pred = eeg_embeddings @ classification_classes_embeds.T\n",
    "\n",
    "                #     # sim_ground = -torch.cdist(text_z.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "                #     # sim_pred = -torch.cdist(eeg_embeddings.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "\n",
    "                #     ground = sim_ground.argmax(dim=1)\n",
    "                #     pred = sim_pred.argmax(dim=1)\n",
    "                #     valid_correct += (ground == pred).sum().item()\n",
    "                #     valid_total_count += ground.shape[0]\n",
    "                    \n",
    "        # avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "        avg_valid_loss = total_valid_loss / valid_count\n",
    "        avg_valid_acc = valid_correct / valid_total_count\n",
    "\n",
    "        if (best_valid_loss is None) or (avg_valid_loss < best_valid_loss):\n",
    "            msg = (f\"Valid Loss: {avg_valid_loss:.10f}\")\n",
    "            log(msg)\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "        msg = (f\"Epoch [{epoch+1}/{epochs}]:- Train Loss: {avg_loss:.6f} | Train Acc: {avg_acc*100:.4f}% | Valid Loss: {avg_valid_loss:.6f} | Valid Acc: {avg_valid_acc*100:.4f}%\")\n",
    "        log(msg)\n",
    "        if (epoch % print_every == 0):\n",
    "            print(msg)\n",
    "        torch.save(model.state_dict(), \"last_model.pt\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e93c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_classes = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "\n",
    "def test(model: nn.Module, test_loader: DataLoader):\n",
    "# def test(model: nn.Module, test_dataset: WordEEGDataset):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    actuals = []\n",
    "    preds = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_loader):\n",
    "        # for batch in tqdm(test_dataset):\n",
    "            # if isinstance(test_dataset, EEGDataset):\n",
    "            #     batch = (batch[0].unsqueeze(0), [batch[1]])\n",
    "            # batch = ([batch[0]], [batch[1]])\n",
    "            B = batch[0].shape[0]\n",
    "            eeg_data = batch[0].to(torch.float32).clone().detach().to(device)\n",
    "            sent_lbl = batch[1].to(torch.int64).clone().detach().to(device)\n",
    "            texts = batch[2]\n",
    "\n",
    "            eeg_embeddings, text_embeddings, sent_logits = model(eeg_data, texts)\n",
    "            sent_logits: torch.Tensor\n",
    "\n",
    "            cosine_loss = 2*(1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean())\n",
    "            ce_loss = loss_fn(sent_logits, sent_lbl)\n",
    "\n",
    "            sent_mask = sent_lbl.view(-1,1) == sent_lbl.view(1,-1)\n",
    "            dist = torch.cdist(eeg_embeddings, eeg_embeddings, p=2)\n",
    "            positive_losses = torch.relu(dist-tau*alpha)\n",
    "            negative_losses = torch.relu(tau*beta-dist)\n",
    "            positive_mask = torch.tril(sent_mask == True, -1)\n",
    "            negative_mask = torch.tril(sent_mask == False, -1)\n",
    "            if positive_mask.sum().item() == 0:\n",
    "                positive_loss = 0\n",
    "            else:\n",
    "                positive_loss = (positive_losses[positive_mask]).sum().item() / (positive_mask.sum().item())\n",
    "            if negative_mask.sum().item() == 0:\n",
    "                negative_loss = 0\n",
    "            else:\n",
    "                negative_loss = (negative_losses[negative_mask]).sum().item() / (negative_mask.sum().item())\n",
    "            clip_loss = positive_loss + negative_loss # clip loss\n",
    "\n",
    "            loss = cosine_loss*loss_weights[0] + ce_loss*loss_weights[1] + clip_loss*loss_weights[2]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "            sent_probs = sent_logits.softmax(dim=1)\n",
    "            sent_preds = sent_probs.argmax(dim=1)\n",
    "            correct += (sent_lbl == sent_preds).sum().item()\n",
    "            total_count += B\n",
    "\n",
    "            actuals.append(sent_lbl)\n",
    "            preds.append(sent_preds)\n",
    "            # for i in range(len(batch[0])):\n",
    "            #     eeg_data = batch[0][i].to(torch.float32).to(device)\n",
    "            #     texts = batch[1][i]\n",
    "\n",
    "            #     eeg_embeddings, text_embeddings = model(eeg_data, texts)\n",
    "\n",
    "            #     # text_z = (text_embeddings-text_mean.unsqueeze(0))/text_std.unsqueeze(0)\n",
    "            #     # loss = loss_fn(eeg_embeddings, text_z)\n",
    "            #     loss = 1.0 - F.cosine_similarity(eeg_embeddings, text_embeddings).mean()\n",
    "\n",
    "            #     total_loss += loss.item()\n",
    "            #     count += 1\n",
    "\n",
    "            #     # Accuracy Check\n",
    "            #     # text_norm = F.normalize(text_embeddings, p=2, dim=1)\n",
    "            #     # eeg_norm = F.normalize(eeg_embeddings, p=2, dim=1)\n",
    "            #     # classes_norm = F.normalize(classification_classes_embeds, p=2, dim=1)\n",
    "            #     sim_ground = text_embeddings @ classification_classes_embeds.T\n",
    "            #     sim_pred = eeg_embeddings @ classification_classes_embeds.T\n",
    "\n",
    "            #     # sim_ground = -torch.cdist(text_z.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "            #     # sim_pred = -torch.cdist(eeg_embeddings.clone().detach(), classification_classes_embeds_z, p=2)\n",
    "\n",
    "            #     ground = sim_ground.argmax(dim=1)\n",
    "            #     pred = sim_pred.argmax(dim=1)\n",
    "            #     correct += (ground == pred).sum().item()\n",
    "            #     total_count += ground.shape[0]\n",
    "            #     actuals.append(ground)\n",
    "            #     preds.append(pred)\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    avg_acc = correct / total_count\n",
    "    print(f\"Test Loss: {avg_loss:.6f} | Test Acc: {avg_acc*100:.4f}%\")\n",
    "\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    cr = classification_report(actuals.cpu(), preds.cpu(), labels=[0,1,2], target_names=classification_classes)\n",
    "    print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b8b1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 842.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 122.25 MiB is free. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 3.17 GiB is allocated by PyTorch, and 83.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# train(model, train_ds, val_ds, epochs=3)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# train(model, ds, epochs=20)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, valid_loader, epochs, print_every)\u001b[39m\n\u001b[32m     53\u001b[39m loss = cosine_loss*loss_weights[\u001b[32m0\u001b[39m] + ce_loss*loss_weights[\u001b[32m1\u001b[39m] + clip_loss*loss_weights[\u001b[32m2\u001b[39m]\n\u001b[32m     55\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m total_loss += loss.item()\n\u001b[32m     59\u001b[39m train_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/optim/adam.py:236\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    233\u001b[39m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m     adam(\n\u001b[32m    247\u001b[39m         params_with_grad,\n\u001b[32m    248\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         decoupled_weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/D/University/Fall 2025/BCI/Project/.venv/lib/python3.13/site-packages/torch/optim/adam.py:180\u001b[39m, in \u001b[36mAdam._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    176\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(\n\u001b[32m    177\u001b[39m     p, memory_format=torch.preserve_format\n\u001b[32m    178\u001b[39m )\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg_sq\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[32m    185\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mmax_exp_avg_sq\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(\n\u001b[32m    186\u001b[39m         p, memory_format=torch.preserve_format\n\u001b[32m    187\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 842.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 122.25 MiB is free. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 3.17 GiB is allocated by PyTorch, and 83.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(model, train_dl, val_dl, epochs=20)\n",
    "# train(model, train_ds, val_ds, epochs=3)\n",
    "# train(model, ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdadb032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.03it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 3.2680826845\n",
      "Epoch [1/100]:- Train Loss: 2.249491 | Train Acc: 59.3501% | Valid Loss: 3.268083 | Valid Acc: 37.6437%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.70it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 3.2081084087\n",
      "Epoch [2/100]:- Train Loss: 2.228060 | Train Acc: 61.1341% | Valid Loss: 3.208108 | Valid Acc: 34.4828%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.69it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 3.1043478045\n",
      "Epoch [3/100]:- Train Loss: 2.203040 | Train Acc: 62.8226% | Valid Loss: 3.104348 | Valid Acc: 38.5057%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 3.0620493396\n",
      "Epoch [4/100]:- Train Loss: 2.233826 | Train Acc: 60.5925% | Valid Loss: 3.062049 | Valid Acc: 35.7759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.73it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100]:- Train Loss: 2.178991 | Train Acc: 62.3447% | Valid Loss: 3.191448 | Valid Acc: 38.0747%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.72it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100]:- Train Loss: 2.197638 | Train Acc: 62.5358% | Valid Loss: 3.196040 | Valid Acc: 38.7931%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.89it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100]:- Train Loss: 2.170930 | Train Acc: 63.1411% | Valid Loss: 3.311077 | Valid Acc: 35.7759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.70it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100]:- Train Loss: 2.141625 | Train Acc: 64.5747% | Valid Loss: 3.390451 | Valid Acc: 35.3448%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.76it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100]:- Train Loss: 2.176647 | Train Acc: 62.6314% | Valid Loss: 3.643703 | Valid Acc: 37.2126%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.77it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]:- Train Loss: 2.151043 | Train Acc: 63.7783% | Valid Loss: 3.409590 | Valid Acc: 34.3391%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100]:- Train Loss: 2.145577 | Train Acc: 63.9694% | Valid Loss: 3.163512 | Valid Acc: 35.9195%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.73it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100]:- Train Loss: 2.158830 | Train Acc: 64.5110% | Valid Loss: 3.442047 | Valid Acc: 36.6379%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.76it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100]:- Train Loss: 2.174996 | Train Acc: 63.4916% | Valid Loss: 3.369437 | Valid Acc: 35.9195%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100]:- Train Loss: 2.157163 | Train Acc: 63.8420% | Valid Loss: 3.392525 | Valid Acc: 36.2069%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.77it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100]:- Train Loss: 2.136640 | Train Acc: 64.2561% | Valid Loss: 3.264354 | Valid Acc: 35.6322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100]:- Train Loss: 2.143131 | Train Acc: 65.0526% | Valid Loss: 3.109866 | Valid Acc: 35.7759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.72it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100]:- Train Loss: 2.123413 | Train Acc: 64.2561% | Valid Loss: 3.374861 | Valid Acc: 35.7759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100]:- Train Loss: 2.134039 | Train Acc: 65.5941% | Valid Loss: 3.320631 | Valid Acc: 36.6379%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.66it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100]:- Train Loss: 2.099697 | Train Acc: 66.0083% | Valid Loss: 3.179503 | Valid Acc: 37.2126%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.64it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100]:- Train Loss: 2.109516 | Train Acc: 66.7729% | Valid Loss: 3.169449 | Valid Acc: 35.0575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.69it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100]:- Train Loss: 2.062069 | Train Acc: 68.4932% | Valid Loss: 3.704993 | Valid Acc: 33.6207%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.78it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100]:- Train Loss: 2.068074 | Train Acc: 67.6967% | Valid Loss: 3.221306 | Valid Acc: 38.6494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100]:- Train Loss: 2.092018 | Train Acc: 68.2064% | Valid Loss: 3.584303 | Valid Acc: 37.2126%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.79it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100]:- Train Loss: 2.083319 | Train Acc: 68.1109% | Valid Loss: 3.475581 | Valid Acc: 37.6437%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.72it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100]:- Train Loss: 2.081147 | Train Acc: 67.3781% | Valid Loss: 3.517997 | Valid Acc: 38.3621%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.66it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100]:- Train Loss: 2.077596 | Train Acc: 66.8047% | Valid Loss: 3.322942 | Valid Acc: 39.3678%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.82it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100]:- Train Loss: 2.046326 | Train Acc: 68.8754% | Valid Loss: 3.360196 | Valid Acc: 38.0747%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.76it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100]:- Train Loss: 2.065541 | Train Acc: 69.6400% | Valid Loss: 3.520713 | Valid Acc: 39.6552%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100]:- Train Loss: 2.059582 | Train Acc: 68.7480% | Valid Loss: 3.385758 | Valid Acc: 37.6437%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100]:- Train Loss: 2.025690 | Train Acc: 70.6913% | Valid Loss: 3.668192 | Valid Acc: 38.2184%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.73it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100]:- Train Loss: 2.043567 | Train Acc: 70.0223% | Valid Loss: 3.419722 | Valid Acc: 38.6494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.83it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100]:- Train Loss: 2.009348 | Train Acc: 70.4683% | Valid Loss: 3.218030 | Valid Acc: 37.6437%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.69it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100]:- Train Loss: 2.011725 | Train Acc: 70.9780% | Valid Loss: 3.358367 | Valid Acc: 35.4885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.68it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100]:- Train Loss: 1.996947 | Train Acc: 72.6983% | Valid Loss: 3.607074 | Valid Acc: 34.3391%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100]:- Train Loss: 2.006346 | Train Acc: 72.1249% | Valid Loss: 3.527680 | Valid Acc: 36.9253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100]:- Train Loss: 1.998595 | Train Acc: 71.3922% | Valid Loss: 3.539766 | Valid Acc: 34.6264%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.68it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100]:- Train Loss: 1.998107 | Train Acc: 71.6470% | Valid Loss: 3.295211 | Valid Acc: 37.3563%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.87it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100]:- Train Loss: 1.999004 | Train Acc: 72.2523% | Valid Loss: 3.565092 | Valid Acc: 37.5000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.75it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100]:- Train Loss: 1.986191 | Train Acc: 73.2399% | Valid Loss: 3.314063 | Valid Acc: 37.6437%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.69it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100]:- Train Loss: 1.944652 | Train Acc: 73.9726% | Valid Loss: 4.255691 | Valid Acc: 35.9195%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.74it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100]:- Train Loss: 1.979765 | Train Acc: 74.4505% | Valid Loss: 3.876803 | Valid Acc: 34.0517%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.72it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100]:- Train Loss: 1.976251 | Train Acc: 73.9407% | Valid Loss: 3.414447 | Valid Acc: 34.4828%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.77it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100]:- Train Loss: 1.946893 | Train Acc: 74.5460% | Valid Loss: 3.513112 | Valid Acc: 35.6322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.65it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100]:- Train Loss: 1.968735 | Train Acc: 73.4629% | Valid Loss: 3.252578 | Valid Acc: 32.6149%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.72it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100]:- Train Loss: 1.966409 | Train Acc: 73.8133% | Valid Loss: 3.197411 | Valid Acc: 36.3506%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.86it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100]:- Train Loss: 1.950946 | Train Acc: 75.9478% | Valid Loss: 3.752260 | Valid Acc: 36.6379%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.29it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100]:- Train Loss: 1.938039 | Train Acc: 75.7566% | Valid Loss: 3.416536 | Valid Acc: 36.7816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.26it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100]:- Train Loss: 1.935113 | Train Acc: 76.2982% | Valid Loss: 4.479050 | Valid Acc: 33.6207%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.32it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100]:- Train Loss: 1.961225 | Train Acc: 75.6610% | Valid Loss: 3.882872 | Valid Acc: 35.3448%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.00it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100]:- Train Loss: 1.932819 | Train Acc: 76.5530% | Valid Loss: 3.405189 | Valid Acc: 34.4828%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100]:- Train Loss: 1.912125 | Train Acc: 77.4450% | Valid Loss: 3.528910 | Valid Acc: 37.7874%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.68it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100]:- Train Loss: 1.915843 | Train Acc: 76.3938% | Valid Loss: 3.560339 | Valid Acc: 33.1897%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.77it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100]:- Train Loss: 1.898035 | Train Acc: 76.5530% | Valid Loss: 3.400937 | Valid Acc: 38.2184%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.62it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100]:- Train Loss: 1.909557 | Train Acc: 76.9990% | Valid Loss: 3.393067 | Valid Acc: 35.7759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.68it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100]:- Train Loss: 1.890379 | Train Acc: 77.3813% | Valid Loss: 3.314253 | Valid Acc: 36.4943%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100]:- Train Loss: 1.891142 | Train Acc: 78.3689% | Valid Loss: 3.440974 | Valid Acc: 35.2011%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.61it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100]:- Train Loss: 1.880001 | Train Acc: 78.4008% | Valid Loss: 3.750726 | Valid Acc: 36.7816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.70it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100]:- Train Loss: 1.895034 | Train Acc: 78.6875% | Valid Loss: 3.726994 | Valid Acc: 36.7816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.69it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100]:- Train Loss: 1.913369 | Train Acc: 78.3052% | Valid Loss: 3.577477 | Valid Acc: 36.7816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.72it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100]:- Train Loss: 1.858537 | Train Acc: 79.9618% | Valid Loss: 3.632579 | Valid Acc: 35.6322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.70it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100]:- Train Loss: 1.884877 | Train Acc: 79.1653% | Valid Loss: 4.065130 | Valid Acc: 34.9138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.78it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100]:- Train Loss: 1.863423 | Train Acc: 79.5795% | Valid Loss: 3.848467 | Valid Acc: 36.2069%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.64it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100]:- Train Loss: 1.881631 | Train Acc: 78.9105% | Valid Loss: 3.611138 | Valid Acc: 38.0747%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.81it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100]:- Train Loss: 1.898721 | Train Acc: 78.7193% | Valid Loss: 3.871513 | Valid Acc: 37.0690%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.82it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100]:- Train Loss: 1.856159 | Train Acc: 80.0892% | Valid Loss: 4.137388 | Valid Acc: 35.6322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.74it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100]:- Train Loss: 1.855988 | Train Acc: 80.9493% | Valid Loss: 4.289869 | Valid Acc: 33.4770%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.02it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100]:- Train Loss: 1.886570 | Train Acc: 79.8662% | Valid Loss: 4.016235 | Valid Acc: 36.7816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.85it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100]:- Train Loss: 1.891394 | Train Acc: 78.9742% | Valid Loss: 3.440410 | Valid Acc: 34.6264%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.65it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100]:- Train Loss: 1.858377 | Train Acc: 80.8856% | Valid Loss: 3.422334 | Valid Acc: 35.4885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100]:- Train Loss: 1.856368 | Train Acc: 80.3441% | Valid Loss: 3.813835 | Valid Acc: 34.9138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.73it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100]:- Train Loss: 1.851678 | Train Acc: 80.4078% | Valid Loss: 3.739020 | Valid Acc: 33.9080%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.80it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100]:- Train Loss: 1.861102 | Train Acc: 80.1211% | Valid Loss: 3.871568 | Valid Acc: 37.5000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100]:- Train Loss: 1.821911 | Train Acc: 81.6821% | Valid Loss: 3.437742 | Valid Acc: 36.2069%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.78it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100]:- Train Loss: 1.844900 | Train Acc: 81.3635% | Valid Loss: 3.613374 | Valid Acc: 35.7759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.66it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100]:- Train Loss: 1.843161 | Train Acc: 81.2361% | Valid Loss: 3.853928 | Valid Acc: 34.6264%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100]:- Train Loss: 1.798637 | Train Acc: 82.0962% | Valid Loss: 4.378736 | Valid Acc: 37.0690%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.19it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100]:- Train Loss: 1.814853 | Train Acc: 83.2749% | Valid Loss: 3.599463 | Valid Acc: 34.7701%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:15<00:00,  8.46it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100]:- Train Loss: 1.859672 | Train Acc: 82.0644% | Valid Loss: 3.719543 | Valid Acc: 36.0632%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.29it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100]:- Train Loss: 1.847012 | Train Acc: 80.2485% | Valid Loss: 4.130186 | Valid Acc: 35.0575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:16<00:00,  8.12it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100]:- Train Loss: 1.828756 | Train Acc: 81.3316% | Valid Loss: 3.867987 | Valid Acc: 35.4885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100]:- Train Loss: 1.781751 | Train Acc: 83.4979% | Valid Loss: 3.721936 | Valid Acc: 34.3391%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.67it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100]:- Train Loss: 1.794812 | Train Acc: 83.2749% | Valid Loss: 4.105492 | Valid Acc: 32.7586%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.73it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100]:- Train Loss: 1.814728 | Train Acc: 82.3829% | Valid Loss: 4.022973 | Valid Acc: 35.3448%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.84it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100]:- Train Loss: 1.801545 | Train Acc: 82.7971% | Valid Loss: 3.738206 | Valid Acc: 33.6207%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.66it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100]:- Train Loss: 1.778513 | Train Acc: 83.2749% | Valid Loss: 3.616576 | Valid Acc: 35.4885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100]:- Train Loss: 1.803291 | Train Acc: 83.4661% | Valid Loss: 3.853360 | Valid Acc: 34.9138%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.75it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100]:- Train Loss: 1.783886 | Train Acc: 83.4979% | Valid Loss: 3.687244 | Valid Acc: 33.9080%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.83it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100]:- Train Loss: 1.807667 | Train Acc: 82.4466% | Valid Loss: 4.165178 | Valid Acc: 33.9080%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.66it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100]:- Train Loss: 1.811709 | Train Acc: 82.8289% | Valid Loss: 3.476168 | Valid Acc: 33.0460%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.66it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100]:- Train Loss: 1.799220 | Train Acc: 83.9758% | Valid Loss: 4.243147 | Valid Acc: 34.7701%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.69it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100]:- Train Loss: 1.778654 | Train Acc: 84.0395% | Valid Loss: 3.934400 | Valid Acc: 35.6322%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.64it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100]:- Train Loss: 1.808064 | Train Acc: 83.3705% | Valid Loss: 4.106045 | Valid Acc: 33.7644%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100]:- Train Loss: 1.802012 | Train Acc: 83.6572% | Valid Loss: 4.274442 | Valid Acc: 33.6207%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.90it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100]:- Train Loss: 1.801616 | Train Acc: 84.3262% | Valid Loss: 4.814084 | Valid Acc: 34.0517%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.57it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100]:- Train Loss: 1.779139 | Train Acc: 85.0908% | Valid Loss: 3.848107 | Valid Acc: 35.0575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.84it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100]:- Train Loss: 1.795931 | Train Acc: 84.1032% | Valid Loss: 3.684386 | Valid Acc: 33.1897%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.71it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100]:- Train Loss: 1.756725 | Train Acc: 85.3138% | Valid Loss: 4.304977 | Valid Acc: 36.7816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.66it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100]:- Train Loss: 1.812097 | Train Acc: 82.9882% | Valid Loss: 3.865751 | Valid Acc: 36.0632%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.62it/s]\n",
      "100%|██████████| 29/29 [00:03<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100]:- Train Loss: 1.768256 | Train Acc: 84.8996% | Valid Loss: 4.064985 | Valid Acc: 32.7586%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:17<00:00,  7.61it/s]\n",
      "100%|██████████| 29/29 [00:02<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100]:- Train Loss: 1.824573 | Train Acc: 83.4342% | Valid Loss: 4.328241 | Valid Acc: 34.4828%\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dl, val_dl, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc782275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.942256 | Test Acc: 37.0370%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.29      0.30      0.30       191\n",
      "     Neutral       0.42      0.50      0.45       271\n",
      "    Negative       0.37      0.28      0.32       240\n",
      "\n",
      "    accuracy                           0.37       702\n",
      "   macro avg       0.36      0.36      0.36       702\n",
      "weighted avg       0.37      0.37      0.37       702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "# model.load_state_dict(torch.load(\"last_model.pt\"))\n",
    "test(model, test_dl)\n",
    "# test(model, test_ds)\n",
    "# test(model, ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
